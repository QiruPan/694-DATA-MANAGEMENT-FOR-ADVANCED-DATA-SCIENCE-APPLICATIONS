{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "936d6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import json\n",
    "import time\n",
    "from mysql.connector import (connection)\n",
    "import mysql.connector\n",
    "from datetime import date\n",
    "\n",
    "class ProjectCache:\n",
    "    \n",
    "    def __init__(self, database):\n",
    "        self.cache = {}                                      #Cache\n",
    "        self.minheap = []                                    #Minheap\n",
    "        self.cache_limit_value = 200                         #The cache will be set up to only hold 200 tweets\n",
    "        self.db = MongoClient(database)[\"my_final_database\"]\n",
    "        self.tweets = self.db[\"tweets\"]\n",
    "        ktdm = list(self.db[\"keywords\"].find())              \n",
    "        htdm = list(self.db[\"hashtags\"].find())\n",
    "        self.keyword_tdm = dict((x['_id'], x['tweets']) for x in ktdm)    #Holds all keywords and ids associated\n",
    "        self.hashtag_tdm = dict((x['_id'], x['tweets']) for x in htdm)    #Holds all hashtags and ids associated\n",
    "        self.total_searches_kept_in_cache = 10      #Each search cache can hold up to 10 most recent unique searches\n",
    "        self.recent_keyword_searches_cache = {}                      \n",
    "        self.recent_hashtag_searches_cache = {}\n",
    "        self.recent_keyword_searches = []\n",
    "        self.recent_hashtag_searches = []\n",
    "        self.connection1 = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"gXt,:RsU#U-ws:3\",database=\"new_database\")\n",
    "        self.users_keyword_cache = {}\n",
    "        self.users_keyword_list = []\n",
    "        self.users_hashtag_cache = {}\n",
    "        self.users_hashtag_list = []\n",
    "        \n",
    "    def __contains__(self, key):\n",
    "        \n",
    "        return key in self.cache\n",
    "    \n",
    "    def current_cache_size(self):\n",
    "        \n",
    "        return len(self.cache)\n",
    "    \n",
    "    def initial_population(self):\n",
    "        \n",
    "        #Read every tweet in collection and attempt to add them one by one to cache\n",
    "        for i in self.tweets:\n",
    "            add_update(i[\"id\"], i)\n",
    "           \n",
    "    def add_update(self, key, value):\n",
    "        \n",
    "        #Add a section that first checks if the current key's relevance value is greater \n",
    "        #than the least relevant term in the cache\n",
    "        if key not in self.cache:\n",
    "            #If the cache limit is met get the least relevant in cache then compare current item with least relevant in cache\n",
    "            if len(self.cache) >= self.cache_limit_value:  \n",
    "                least_relevant_term()                                       \n",
    "                if value[\"relevance\"] > least_relevant_value_in_cache:      \n",
    "                    \n",
    "                    #then remove least relevant to make room in cache\n",
    "                    self.remove_least_relevant(least_relevant_key, least_relevant_value_in_cache)          \n",
    "                    \n",
    "                    #Add key and relevance value to minheap list\n",
    "                    minheap_sort(key, value[\"relevance\"])\n",
    "                    \n",
    "                    #Then add key and all tweet elements to cache\n",
    "                    self.cache[key] = value                  \n",
    "        \n",
    "            else:\n",
    "                #Add key and relevance value to minheap list\n",
    "                minheap_sort(key, value[\"relevance\"])\n",
    "                \n",
    "                #Then add key and all tweet elements to cache\n",
    "                self.cache[key] = value\n",
    "        \n",
    "    def remove_least_relevant(self, least_relevant_key, least_relevant_value_in_cache):\n",
    "        \n",
    "        #Remove least relevant from both the cache and the heaped list\n",
    "        self.cache.pop(least_relevant)\n",
    "        heappop(self.minheap, (least_relevant_value_in_cache, least_relevant))\n",
    "        heapify(self.minheap)\n",
    "        \n",
    "    def least_relevant_term(self):\n",
    "        \n",
    "        #Get the least relevant term within the cache as well as key associated with it.      \n",
    "        least_relevant_value_in_cache = self.minheap[0][0]\n",
    "        least_relevant_key = self.minheap[0][1]\n",
    "        return least_relevant_value_in_cache, least_relevant_key\n",
    "    \n",
    "    def minheap_sort(self, key, value):\n",
    "        \n",
    "        #The relevance will be the new key, and the key will be the value in the minheap.      \n",
    "        heappush(self.minheap, (value, key))\n",
    "        heapify(self.minheap)\n",
    "    \n",
    "    \n",
    "    def keyword(self, word, querytype):\n",
    "        \n",
    "        #Make a blank dictionary\n",
    "        l={}\n",
    "        \n",
    "        #Grab all the tweet ids associated with the word/hashtag\n",
    "        if querytype == \"Text\":\n",
    "            #If word is in searched cache just return that dictionary\n",
    "            if word in self.recent_keyword_searches_cache:\n",
    "                self.recent_keyword_searches.remove(word)\n",
    "                self.recent_keyword_searches.append(word)\n",
    "                l = self.recent_keyword_searches_cache[word]\n",
    "                return pd.DataFrame.from_dict(l).transpose()\n",
    "            else:\n",
    "                ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
    "        elif querytype == \"Hashtag\":\n",
    "            #If word is in searched cache just return that dictionary\n",
    "            if word in self.recent_hashtag_searches_cache:\n",
    "                self.recent_hashtag_searches.remove(word)\n",
    "                self.recent_hashtag_searches.append(word)\n",
    "                l = self.recent_hashtag_searches_cache[word]\n",
    "                return pd.DataFrame.from_dict(l).transpose()\n",
    "            else:\n",
    "                ids = self.hashtag_tdm[word]\n",
    "        \n",
    "        #Search through cache first for all tweet ids\n",
    "        #If it's not in the cache find it in the database\n",
    "        for i in ids:\n",
    "            if i in self.cache:\n",
    "                l[i] = self.cache[i]\n",
    "            else:\n",
    "                l[i] = self.connection_name.find({\"id\":i})\n",
    "        \n",
    "        #make a nested dictionary with the three most recent searched words saved in a queue\n",
    "        if querytype == \"Text\":\n",
    "            self.recent_keyword_searches_cache[word] = l\n",
    "            self.recent_keyword_searches.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.recent_keyword_searches_cache) > self.total_searches_kept_in_cache:\n",
    "                a = self.recent_keyword_searches.pop(0)\n",
    "                self.recent_keyword_searches_cache.pop(a)\n",
    "        elif querytype == \"Hashtag\":\n",
    "            self.recent_hashtag_searches_cache[word] = l\n",
    "            self.recent_hashtag_searches.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.recent_hashtag_searches_cache) > self.total_searches_kept_in_cache:\n",
    "                a = self.recent_hashtag_searches.pop(0)\n",
    "                self.recent_hashtag_searches_cache.pop(a)\n",
    "            \n",
    "        l = pd.DataFrame.from_dict(l).transpose()\n",
    "        \n",
    "        #Return a dictionary of all of the ids and associated tweets\n",
    "        return l, word, querytype\n",
    "        \n",
    "    def userlist(self, l, word, querytype):\n",
    "        \n",
    "        #returns users already in text or hashtag cache\n",
    "        if querytype == \"Text\":\n",
    "            if word in self.users_keyword_cache:\n",
    "                userCounts = self.users_keyword_cache[word]\n",
    "                return userCounts.head(5)\n",
    "            \n",
    "        elif querytype == \"Hashtag\":\n",
    "            if word in self.users_keyword_cache:\n",
    "                userCounts = self.users_keyword_cache[word]\n",
    "                return userCounts.head(5)\n",
    "            \n",
    "        userCounts = l.user.value_counts().to_frame(name = 'relevant_tweets')\n",
    "        userCounts['user_id'] = list(userCounts.index)\n",
    "     \n",
    "        #Query mySQL for users not found in cache\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM users_table \n",
    "        WHERE id IN (%s)\n",
    "        \"\"\" % ', '.join(str(x) for x in userCounts.user_id)\n",
    "              \n",
    "        userList = pd.read_sql(query, connection1)\n",
    "        \n",
    "        #Merge user_id of tweets with id of users\n",
    "        userCounts = userCounts.merge(userList, left_on = 'user_id', right_on = 'id')\n",
    "        \n",
    "        userCounts = userCounts.sort_values(by=['relevant_tweets', 'followers_count', 'friends_count'], ascending = False)\n",
    "        \n",
    "        #Add word used in search to cache for users\n",
    "        if querytype == \"Text\":    \n",
    "            self.users_keyword_cache[word] = userCounts\n",
    "            self.users_keyword_list.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.users_keyword_list) > self.total_searches_kept_in_cache:\n",
    "                a = self.users_keyword_list.pop(0)\n",
    "                self.users_keyword_cache.pop(a)\n",
    "                \n",
    "        elif querytype == \"Hashtag\":\n",
    "            self.users_hashtag_cache[word] = userCounts\n",
    "            self.users_hashtag_list.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.users_hashtag_list) > self.total_searches_kept_in_cache:\n",
    "                a = self.users_hashtag_list.pop(0)\n",
    "                self.users_hashtag_cache.pop(a)\n",
    "                \n",
    "        #Returns the five most active users\n",
    "        return userCounts.head(5)\n",
    "    \n",
    "    def user(self, username):\n",
    "        #Make a blank dictionary\n",
    "        l={}\n",
    "        \n",
    "        #Grab all the tweet ids associated with the word/hashtag\n",
    "        #If word is in searched cache just return that dictionary\n",
    "        if username in self.recent_user_searches_cache:\n",
    "            self.recent_user_searches.remove(username)\n",
    "            self.recent_user_searches.append(username)\n",
    "            l = self.recent_user_searches_cache[username]\n",
    "            return pd.DataFrame.from_dict(l).transpose()\n",
    "        else:\n",
    "            query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM new_database.users_table \n",
    "            WHERE name LIKE '%{}%' OR screen_name LIKE '%{}%'\n",
    "            \"\"\".format(username, username)\n",
    "            #Query mySQL for users not found in cache\n",
    "            userList = pd.read_sql(query, self.connection1)\n",
    "            \n",
    "            userList = userList.sort_values(by=['followers_count', 'friends_count'], ascending = False)\n",
    "            \n",
    "            self.recent_user_searches_cache[username] = userList\n",
    "            self.recent_user_searches.append(username)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.recent_user_searches_cache) > self.total_searches_kept_in_cache:\n",
    "                a = self.recent_user_searches.pop(0)\n",
    "                self.recent_user_searches_cache.pop(a)\n",
    "            return userList.head(5)\n",
    "       \n",
    "        \n",
    "    def tweetlist(self, userlist, username):\n",
    "        \n",
    "        #If username in recent search list return tweets associated to user\n",
    "        if username in self.tweets_user_cache:\n",
    "            self.tweets_user_list.remove(username)\n",
    "            self.tweets_user_list.append(username)\n",
    "            l = self.tweets_user_cache[username]\n",
    "            return pd.DataFrame.from_dict(l).transpose()\n",
    "        \n",
    "        tweet_ids = [x for sl in [x.split(',') for x in userlist['tweets']] for x in sl]\n",
    "        #Make a blank dictionary\n",
    "        l={}\n",
    "        \n",
    "        #Search through cache first for all tweet ids\n",
    "        #If it's not in the cache find it in the database\n",
    "        for i in tweet_ids:\n",
    "            if i in self.cache:\n",
    "                l[i] = self.cache[i]\n",
    "            else:\n",
    "                l[i] = self.tweets.find_one({\"_id\":int(i)})\n",
    "        \n",
    "        self.tweets_user_cache[username] = l\n",
    "        self.tweets_user_list.append(username)\n",
    "        #If the queue is larger than the total accepted searches remove \n",
    "        if len(self.tweets_user_cache) > self.total_searches_kept_in_cache:\n",
    "            a = self.tweets_user_list.pop(0)\n",
    "            self.tweets_user_cache.pop(a)\n",
    "          \n",
    "        return pd.DataFrame.from_dict(l).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ab681a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc, callback, Output, Input, dash_table\n",
    "from dash.dependencies import State\n",
    "import pandas as pd\n",
    "\n",
    "# import dash_bootstrap_components as dbc\n",
    "\n",
    "# Some default set of tweets (could be 1) - the relevant part is the column headers\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminder_unfiltered.csv')\n",
    "\n",
    "database = \"mongodb://localhost:27017\"\n",
    "\n",
    "PC = ProjectCache(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:8050\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-datepicker.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\u001b[0m\" 304 -\n",
      "[2023-04-28 20:06:29,422] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 2528, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/dash.py\", line 1283, in dispatch\n",
      "    ctx.run(\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/_callback.py\", line 450, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_17745/1897438089.py\", line 79, in update_table\n",
      "    dff = PC.keyword(value, stype)\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_17745/555349630.py\", line 177, in keyword\n",
      "    ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
      "KeyError: None\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:29] \"\u001b[35m\u001b[1mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 500 -\n",
      "/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_17745/555349630.py:130: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n",
      "127.0.0.1 - - [28/Apr/2023 20:06:34] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u1\n",
      "1682726794.866834\n",
      "u2\n",
      "1682726794.8669162\n",
      "u3\n",
      "1682726794.8826041\n",
      "u4\n",
      "1682726794.882615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Apr/2023 20:06:47] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from dash import Dash, html, dcc, callback\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_table\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "search_output_columns = [{\"name\": i, \"id\": i, \"type\": \"text\", \"presentation\": \"markdown\"} for i in ['user_name', 'created_at', 'text', 'reply_count', 'retweet_count']]\n",
    "search_output_columns[2]['style'] = {'maxWidth': '200px', 'overflow': 'hidden', 'textOverflow': 'ellipsis'}\n",
    "\n",
    "user_df_columns = [{\"name\": i, \"id\": i, \"type\": \"text\", \"presentation\": \"markdown\"} for i in ['name', 'screen_name', 'relevant_tweets', 'created_at', 'followers_count', 'friends_count', 'verified']]\n",
    "user_df_columns[2]['style'] = {'maxWidth': '200px', 'overflow': 'hidden', 'textOverflow': 'ellipsis'}\n",
    "\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(children='Tweet Search', style={'textAlign':'center'}),\n",
    "    dcc.RadioItems(['Text', 'Hashtag','User'], 'Text', id='search_selection', inline=True),\n",
    "    dcc.Input(id=\"search_query\", type=\"text\", placeholder=\"search query\"),\n",
    "    dcc.DatePickerSingle(\n",
    "        id='start_date',\n",
    "        min_date_allowed=date(2020, 4, 25),\n",
    "        max_date_allowed=date(2020, 4, 25),\n",
    "        initial_visible_month=date(2020, 4, 25),\n",
    "        date=date(2020, 4, 25),\n",
    "        display_format='MMM Do, YY',\n",
    "        placeholder='MMM Do, YY'\n",
    "    ),\n",
    "    dcc.Input(id=\"start_time\", type=\"time\", value=\"12:21:41\"),\n",
    "    dcc.DatePickerSingle(\n",
    "        id='end_date',\n",
    "        min_date_allowed=date(2020, 4, 25),\n",
    "        max_date_allowed=date(2020, 4, 25),\n",
    "        initial_visible_month=date(2020, 4, 25),\n",
    "        date=date(2020, 4, 25),\n",
    "        display_format='MMM Do, YY',\n",
    "        placeholder='MMM Do, YY'\n",
    "    ),\n",
    "    dcc.Input(id=\"end_time\", type=\"time\", value=\"14:48:38\"),\n",
    "    dcc.Checklist(['Filter by Time'], ['Filter by Time'], inline=True, id=\"time_search\"),\n",
    "    html.Button('Search', id='search_submit'),\n",
    "    html.Div(id='search_time'),\n",
    "    html.H2(children='Output'),\n",
    "    dash_table.DataTable(\n",
    "        data=[],\n",
    "        columns=search_output_columns,\n",
    "        id='search_output',\n",
    "        style_cell={'whiteSpace': 'normal', 'height': 'auto'},\n",
    "        style_data_conditional=[{'whiteSpace': 'normal', 'height': 'auto'}],\n",
    "        page_size=10\n",
    "    ),\n",
    "    html.H2(children='Users'),\n",
    "    dash_table.DataTable(\n",
    "        data=pd.DataFrame().to_dict('records'),\n",
    "        columns=user_df_columns,\n",
    "        id='user_df',\n",
    "        style_cell={'whiteSpace': 'normal', 'height': 'auto'},\n",
    "        style_data_conditional=[{'whiteSpace': 'normal', 'height': 'auto'}],\n",
    "        page_size=10\n",
    "    )\n",
    "])\n",
    "\n",
    "@callback(\n",
    "    Output('search_output', 'data'),\n",
    "    Output('user_df', 'data'),\n",
    "    Output('search_time', 'children'),\n",
    "    State('search_selection', 'value'),\n",
    "    State('search_query', 'value'),\n",
    "    State('start_date', 'value'),\n",
    "    State('start_time', 'value'),\n",
    "    State('end_date', 'value'),\n",
    "    State('end_time', 'value'),\n",
    "    Input('search_submit', 'n_clicks')\n",
    ")\n",
    "def update_table(stype, value, start_date, start_time, end_date, end_time, n):\n",
    "    start = time.time()\n",
    "    \n",
    "    if stype != \"User\":\n",
    "        \n",
    "        dff = PC.keyword(value, stype)\n",
    "        \n",
    "        # Filter the search results based on the time range\n",
    "        if start_time and end_time:\n",
    "            start_datetime = pd.to_datetime(start_time, format='%H:%M:%S').time()\n",
    "            end_datetime = pd.to_datetime(end_time, format='%H:%M:%S').time()\n",
    "            dff = dff[(dff['created_at'].dt.time >= start_datetime) & (dff['created_at'].dt.time <= end_datetime)]\n",
    "\n",
    "        dff2 = PC.userlist(dff, value, stype)\n",
    "        \n",
    "        userCounts = dff.user.value_counts().to_frame(name = 'relevant_tweets')\n",
    "        userCounts['user_id'] = list(userCounts.index)\n",
    "        \n",
    "        dff2 = dff2.merge(userCounts, left_on = 'id', right_on = 'user_id')\n",
    "        \n",
    "        # Sort the user dataframe by followers_count, verified, and friends_count\n",
    "        dff2 = dff2.sort_values(['relevant_tweets','followers_count', 'verified', 'friends_count'], ascending=[False, False, False, False])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        dff2 = PC.user(value)\n",
    "        \n",
    "        dff = PC.tweetlist(dff2, value)\n",
    "        \n",
    "        # Filter the search results based on the time range\n",
    "        if start_time and end_time:\n",
    "            start_datetime = pd.to_datetime(start_time, format='%H:%M:%S').time()\n",
    "            end_datetime = pd.to_datetime(end_time, format='%H:%M:%S').time()\n",
    "            dff = dff[(dff['created_at'].dt.time >= start_datetime) & (dff['created_at'].dt.time <= end_datetime)]\n",
    "            \n",
    "        # Sort the user dataframe by followers_count, verified, and friends_count\n",
    "        dff2 = dff2.sort_values(['followers_count', 'verified', 'friends_count'], ascending=[False, False, False])\n",
    "        \n",
    "        dff2['relevant_tweets'] = \"Not Applicable\"\n",
    "\n",
    "    \n",
    "    # Sort the search results based on reply_count and retweet_count\n",
    "    dff = dff.sort_values(['reply_count', 'retweet_count'], ascending=[False, False])\n",
    "\n",
    "    end = time.time()\n",
    "    return dff.reset_index().to_dict(\"records\"), dff2.reset_index().to_dict(\"records\"), \"Search Time: {} s\".format(round(end-start, 3))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085656fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf814a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
