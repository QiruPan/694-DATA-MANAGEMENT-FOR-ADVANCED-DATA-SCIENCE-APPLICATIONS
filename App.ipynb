{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "936d6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import json\n",
    "import time\n",
    "\n",
    "class ProjectCache:\n",
    "    \n",
    "    def __init__(self, database):\n",
    "        self.cache = {}                                      #Cache\n",
    "        self.minheap = []                                    #Minheap\n",
    "        self.cache_limit_value = 200                         #The cache will be set up to only hold 200 tweets\n",
    "        self.db = MongoClient(database)[\"TwitterDB\"]\n",
    "        self.tweets = self.db[\"tweets\"]\n",
    "        ktdm = list(self.db[\"keywords\"].find())  #Holds all keywords and ids associated\n",
    "        self.keyword_tdm = dict((x['_id'], x['tweets']) for x in ktdm)\n",
    "        htdm = list(self.db[\"hashtags\"].find())\n",
    "        self.hashtag_tdm = dict((x['_id'], x['tweets']) for x in htdm)              #Holds all hashtags and ids associated\n",
    "        self.recent_searches = []                            #Currently not in use\n",
    "        self.total_searches_kept_in_cache = 5                #Currently not in use\n",
    "        \n",
    "        \n",
    "    def __contains__(self, key):\n",
    "        \n",
    "        return key in self.cache\n",
    "    \n",
    "    def current_cache_size(self):\n",
    "        \n",
    "        return len(self.cache)\n",
    "    \n",
    "    def initial_population(self):\n",
    "        \n",
    "        #Read every tweet in collection and attempt to add them one by one to cache\n",
    "        for i in self.tweets:\n",
    "            add_update(i[\"id\"], i)\n",
    "           \n",
    "    def add_update(self, key, value):\n",
    "        \n",
    "        #Add a section that first checks if the current key's relevance value is greater \n",
    "        #than the least relevant term in the cache\n",
    "        if key not in self.cache:\n",
    "            #If the cache limit is met get the least relevant in cache then compare current item with least relevant in cache\n",
    "            if len(self.cache) >= self.cache_limit_value:  \n",
    "                least_relevant_term()                                       \n",
    "                if value[\"relevance\"] > least_relevant_value_in_cache:      \n",
    "                    \n",
    "                    #then remove least relevant to make room in cache\n",
    "                    self.remove_least_relevant(least_relevant_key, least_relevant_value_in_cache)          \n",
    "                    \n",
    "                    #Add key and relevance value to minheap list\n",
    "                    minheap_sort(key, value[\"relevance\"])\n",
    "                    \n",
    "                    #Then add key and all tweet elements to cache\n",
    "                    self.cache[key] = value                  \n",
    "        \n",
    "            else:\n",
    "                #Add key and relevance value to minheap list\n",
    "                minheap_sort(key, value[\"relevance\"])\n",
    "                \n",
    "                #Then add key and all tweet elements to cache\n",
    "                self.cache[key] = value\n",
    "        \n",
    "    def remove_least_relevant(self, least_relevant_key, least_relevant_value_in_cache):\n",
    "        \n",
    "        #Remove least relevant from both the cache and the heaped list\n",
    "        self.cache.pop(least_relevant)\n",
    "        heappop(self.minheap, (least_relevant_value_in_cache, least_relevant))\n",
    "        heapify(self.minheap)\n",
    "        \n",
    "    def least_relevant_term(self):\n",
    "        \n",
    "        #Get the least relevant term within the cache as well as key associated with it.      \n",
    "        least_relevant_value_in_cache = self.minheap[0][0]\n",
    "        least_relevant_key = self.minheap[0][1]\n",
    "        return least_relevant_value_in_cache, least_relevant_key\n",
    "    \n",
    "    def minheap_sort(self, key, value):\n",
    "        \n",
    "        #The relevance will be the new key, and the key will be the value in the minheap.      \n",
    "        heappush(self.minheap, (value, key))\n",
    "        heapify(self.minheap)\n",
    "    \n",
    "    def keyword(self, word, querytype):\n",
    "        \n",
    "        #Make a blank dictionary\n",
    "        l={}\n",
    "        \n",
    "        #Grab all the tweet ids associated with the word/hashtag\n",
    "        if querytype == \"Text\":\n",
    "            ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
    "        elif querytype == \"Hashtag\":\n",
    "            ids = self.hashtag_tdm[word]\n",
    "        \n",
    "        #Search through cache first for all tweet ids\n",
    "        #If it's not in the cache find it in the database\n",
    "        for i in ids:\n",
    "            if i in self.cache:\n",
    "                l[i] = self.cache[i]\n",
    "            else:\n",
    "                l[i] = self.tweets.find_one({\"_id\":i})\n",
    "                \n",
    "        l = pd.DataFrame.from_dict(l).transpose()\n",
    "        \n",
    "        #Return a dictionary of all of the ids and associated tweets\n",
    "        return l\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ab681a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc, callback, Output, Input, dash_table\n",
    "from dash.dependencies import State\n",
    "import pandas as pd\n",
    "\n",
    "# import dash_bootstrap_components as dbc\n",
    "\n",
    "# Some default set of tweets (could be 1) - the relevant part is the column headers\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminder_unfiltered.csv')\n",
    "\n",
    "database = \"mongodb://localhost:27017\"\n",
    "\n",
    "PC = ProjectCache(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:8050\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:47] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:47] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "[2023-04-24 17:04:47,114] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 2528, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/dash.py\", line 1283, in dispatch\n",
      "    ctx.run(\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/_callback.py\", line 450, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_88711/2769079674.py\", line 28, in update_table\n",
      "    dff = PC.keyword(value, stype)\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_88711/2920537983.py\", line 89, in keyword\n",
      "    ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
      "KeyError: None\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:47] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:47] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:47] \"\u001b[35m\u001b[1mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Apr/2023 17:04:58] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(children='Tweet Search', style={'textAlign':'center'}),\n",
    "    dcc.RadioItems(['Text', 'Hashtag'], 'Text', id='search_selection', inline=True),\n",
    "    dcc.Input(id=\"search_query\", type=\"text\", placeholder=\"search query\"),\n",
    "    html.Button('Search', id='search_submit'),\n",
    "    html.Div(id='search_time'),\n",
    "    html.H2(children='Output'),\n",
    "    dash_table.DataTable(data=pd.DataFrame().to_dict('records'),\n",
    "    columns=[{\"name\": i, \"id\": i} for i in ['user_name', 'created_at', 'text', 'reply_count', 'retweet_count']], id='search_output'),\n",
    "    html.H2(children='Similar'),\n",
    "    dash_table.DataTable(data=pd.DataFrame().to_dict('records'),\n",
    "    columns=[{\"name\": i, \"id\": i} for i in pd.DataFrame().columns], id='similar_df')\n",
    "])\n",
    "\n",
    "@callback(\n",
    "    Output('search_output', 'data'),\n",
    "    Output('similar_df', 'data'),\n",
    "    Output('search_time', 'children'),\n",
    "    State('search_selection', 'value'),\n",
    "    State('search_query', 'value'),\n",
    "    Input('search_submit', 'n_clicks')\n",
    ") # This function should be updated to refer to and call tweets as appropriate\n",
    "def update_table(stype, value, n):\n",
    "    start = time.time()\n",
    "    \n",
    "    dff = PC.keyword(value, stype)\n",
    "    dff2 = dff\n",
    "    \n",
    "    end = time.time()\n",
    "    return dff.reset_index().to_dict(\"records\"), dff2.reset_index().to_dict(\"records\"), \"Search Time: {} s\".format(end-start, 3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85479c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
