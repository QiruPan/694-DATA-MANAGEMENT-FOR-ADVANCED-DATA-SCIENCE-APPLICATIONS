{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "936d6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import json\n",
    "import time\n",
    "from mysql.connector import (connection)\n",
    "import mysql.connector\n",
    "\n",
    "class ProjectCache:\n",
    "    \n",
    "    def __init__(self, database):\n",
    "        self.cache = {}                                      #Cache\n",
    "        self.minheap = []                                    #Minheap\n",
    "        self.cache_limit_value = 200                         #The cache will be set up to only hold 200 tweets\n",
    "        self.db = MongoClient(database)[\"TwitterDB\"]\n",
    "        self.tweets = self.db[\"tweets\"]\n",
    "        ktdm = list(self.db[\"keywords\"].find())  #Holds all keywords and ids associated\n",
    "        self.keyword_tdm = dict((x['_id'], x['tweets']) for x in ktdm)\n",
    "        htdm = list(self.db[\"hashtags\"].find())\n",
    "        self.hashtag_tdm = dict((x['_id'], x['tweets']) for x in htdm)              #Holds all hashtags and ids associated\n",
    "        self.recent_keyword_searches_cache = {}                      \n",
    "        self.recent_hashtag_searches_cache = {}\n",
    "        self.total_searches_kept_in_cache = 3                        \n",
    "        self.recent_keyword_searches = []\n",
    "        self.recent_hashtag_searches = []\n",
    "        self.connection1 = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"gXt,:RsU#U-ws:3\",database=\"new_database\")\n",
    "        \n",
    "        \n",
    "    def __contains__(self, key):\n",
    "        \n",
    "        return key in self.cache\n",
    "    \n",
    "    def current_cache_size(self):\n",
    "        \n",
    "        return len(self.cache)\n",
    "    \n",
    "    def initial_population(self):\n",
    "        \n",
    "        #Read every tweet in collection and attempt to add them one by one to cache\n",
    "        for i in self.tweets:\n",
    "            add_update(i[\"id\"], i)\n",
    "           \n",
    "    def add_update(self, key, value):\n",
    "        \n",
    "        #Add a section that first checks if the current key's relevance value is greater \n",
    "        #than the least relevant term in the cache\n",
    "        if key not in self.cache:\n",
    "            #If the cache limit is met get the least relevant in cache then compare current item with least relevant in cache\n",
    "            if len(self.cache) >= self.cache_limit_value:  \n",
    "                least_relevant_term()                                       \n",
    "                if value[\"relevance\"] > least_relevant_value_in_cache:      \n",
    "                    \n",
    "                    #then remove least relevant to make room in cache\n",
    "                    self.remove_least_relevant(least_relevant_key, least_relevant_value_in_cache)          \n",
    "                    \n",
    "                    #Add key and relevance value to minheap list\n",
    "                    minheap_sort(key, value[\"relevance\"])\n",
    "                    \n",
    "                    #Then add key and all tweet elements to cache\n",
    "                    self.cache[key] = value                  \n",
    "        \n",
    "            else:\n",
    "                #Add key and relevance value to minheap list\n",
    "                minheap_sort(key, value[\"relevance\"])\n",
    "                \n",
    "                #Then add key and all tweet elements to cache\n",
    "                self.cache[key] = value\n",
    "        \n",
    "    def remove_least_relevant(self, least_relevant_key, least_relevant_value_in_cache):\n",
    "        \n",
    "        #Remove least relevant from both the cache and the heaped list\n",
    "        self.cache.pop(least_relevant)\n",
    "        heappop(self.minheap, (least_relevant_value_in_cache, least_relevant))\n",
    "        heapify(self.minheap)\n",
    "        \n",
    "    def least_relevant_term(self):\n",
    "        \n",
    "        #Get the least relevant term within the cache as well as key associated with it.      \n",
    "        least_relevant_value_in_cache = self.minheap[0][0]\n",
    "        least_relevant_key = self.minheap[0][1]\n",
    "        return least_relevant_value_in_cache, least_relevant_key\n",
    "    \n",
    "    def minheap_sort(self, key, value):\n",
    "        \n",
    "        #The relevance will be the new key, and the key will be the value in the minheap.      \n",
    "        heappush(self.minheap, (value, key))\n",
    "        heapify(self.minheap)\n",
    "    \n",
    "    def userlist(self, l):\n",
    "        \n",
    "        userCounts = l.user.value_counts().to_frame(name = 'relevant_tweets')\n",
    "        userCounts['user_id'] = list(userCounts.index)\n",
    "        \n",
    "        print('u1')\n",
    "        print(time.time())\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM new_database.users_table \n",
    "        WHERE id IN (%s)\n",
    "        \"\"\" % ', '.join(str(x) for x in userCounts.user_id)\n",
    "        \n",
    "        print('u2')\n",
    "        print(time.time())\n",
    "        \n",
    "        userList = pd.read_sql(query, connection1)\n",
    "        \n",
    "        print('u3')\n",
    "        print(time.time())\n",
    "        \n",
    "        userCounts = userCounts.merge(userList, left_on = 'user_id', right_on = 'id')\n",
    "        \n",
    "        userCounts = userCounts.sort_values(by=['relevant_tweets', 'followers_count', 'friends_count'], ascending = False)\n",
    "        \n",
    "        print('u4')\n",
    "        print(time.time())\n",
    "        \n",
    "        return userCounts.head(5)\n",
    "        \n",
    "    \n",
    "    def keyword(self, word, querytype):\n",
    "        \n",
    "        #Make a blank dictionary\n",
    "        l={}\n",
    "        \n",
    "        #Grab all the tweet ids associated with the word/hashtag\n",
    "        if querytype == \"Text\":\n",
    "            #If word is in searched cache just return that dictionary\n",
    "            if word in self.recent_keyword_searches_cache:\n",
    "                self.recent_keyword_searches.remove(word)\n",
    "                self.recent_keyword_searches.append(word)\n",
    "                l = self.recent_keyword_searches_cache[word]\n",
    "                return pd.DataFrame.from_dict(l).transpose()\n",
    "            else:\n",
    "                ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
    "        elif querytype == \"Hashtag\":\n",
    "            #If word is in searched cache just return that dictionary\n",
    "            if word in self.recent_hashtag_searches_cache:\n",
    "                self.recent_hashtag_searches.remove(word)\n",
    "                self.recent_hashtag_searches.append(word)\n",
    "                l = self.recent_hashtag_searches_cache[word]\n",
    "                return pd.DataFrame.from_dict(l).transpose()\n",
    "            else:\n",
    "                ids = self.hashtag_tdm[word]\n",
    "        \n",
    "        #Search through cache first for all tweet ids\n",
    "        #If it's not in the cache find it in the database\n",
    "        for i in ids:\n",
    "            if i in self.cache:\n",
    "                l[i] = self.cache[i]\n",
    "            else:\n",
    "                l[i] = self.tweets.find_one({\"_id\":i})\n",
    "                \n",
    "        #make a nested dictionary with the three most recent searched words saved in a queue\n",
    "        if querytype == \"Text\":\n",
    "            self.recent_keyword_searches_cache[word] = l\n",
    "            self.recent_keyword_searches.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.recent_keyword_searches_cache) > self.total_searches_kept_in_cache:\n",
    "                a = self.recent_keyword_searches.pop(0)\n",
    "                self.recent_keyword_searches_cache.pop(a)\n",
    "        elif querytype == \"Hashtag\":\n",
    "            self.recent_hashtag_searches_cache[word] = l\n",
    "            self.recent_hashtag_searches.append(word)\n",
    "            #If the queue is larger than the total accepted searches remove \n",
    "            if len(self.recent_hashtag_searches_cache) > self.total_searches_kept_in_cache:\n",
    "                a = self.recent_hashtag_searches.pop(0)\n",
    "                self.recent_hashtag_searches_cache.pop(a)\n",
    "        \n",
    "        l = pd.DataFrame.from_dict(l).transpose()\n",
    "        \n",
    "        #Return a dictionary of all of the ids and associated tweets\n",
    "        return l\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ab681a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc, callback, Output, Input, dash_table\n",
    "from dash.dependencies import State\n",
    "import pandas as pd\n",
    "\n",
    "# import dash_bootstrap_components as dbc\n",
    "\n",
    "# Some default set of tweets (could be 1) - the relevant part is the column headers\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminder_unfiltered.csv')\n",
    "\n",
    "database = \"mongodb://localhost:27017\"\n",
    "\n",
    "PC = ProjectCache(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:8050\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\u001b[0m\" 304 -\n",
      "[2023-04-28 00:59:42,431] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 2528, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/flask/app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/dash.py\", line 1283, in dispatch\n",
      "    ctx.run(\n",
      "  File \"/Users/mazinrafi/miniconda3/lib/python3.10/site-packages/dash/_callback.py\", line 450, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_78598/2786429339.py\", line 34, in update_table\n",
      "    dff = PC.keyword(value, stype)\n",
      "  File \"/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_78598/2543684043.py\", line 134, in keyword\n",
      "    ids = self.keyword_tdm[word] #Could use .head(10) to get only the top 10\n",
      "KeyError: None\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"\u001b[36mGET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:42] \"\u001b[35m\u001b[1mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682657982.4287202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_78598/2543684043.py:105: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682657984.429358\n",
      "Got tweets\n",
      "1682657984.4439502\n",
      "u1\n",
      "1682657984.451583\n",
      "u2\n",
      "1682657984.4517472\n",
      "u3\n",
      "1682657984.485827\n",
      "u4\n",
      "1682657984.493749\n",
      "Got users\n",
      "1682657984.493818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/17szgjb53c52s1rvb1qj14q00000gn/T/ipykernel_78598/2543684043.py:105: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n",
      "127.0.0.1 - - [28/Apr/2023 00:59:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682657999.675273\n",
      "Got tweets\n",
      "1682657999.6945999\n",
      "u1\n",
      "1682657999.697773\n",
      "u2\n",
      "1682657999.698171\n",
      "u3\n",
      "1682657999.732918\n",
      "u4\n",
      "1682657999.7369988\n",
      "Got users\n",
      "1682657999.737157\n"
     ]
    }
   ],
   "source": [
    "from dash import Dash, html, dcc, callback\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_table\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "search_output_columns = [{\"name\": i, \"id\": i, \"type\": \"text\", \"presentation\": \"markdown\"} for i in ['user_name', 'created_at', 'text', 'reply_count', 'retweet_count']]\n",
    "search_output_columns[2]['style'] = {'maxWidth': '200px', 'overflow': 'hidden', 'textOverflow': 'ellipsis'}\n",
    "\n",
    "user_df_columns = [{\"name\": i, \"id\": i, \"type\": \"text\", \"presentation\": \"markdown\"} for i in ['name', 'screen_name', 'relevant_tweets', 'created_at', 'followers_count', 'friends_count', 'verified']]\n",
    "user_df_columns[2]['style'] = {'maxWidth': '200px', 'overflow': 'hidden', 'textOverflow': 'ellipsis'}\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(children='Tweet Search', style={'textAlign':'center'}),\n",
    "    dcc.RadioItems(['Text', 'Hashtag'], 'Text', id='search_selection', inline=True),\n",
    "    dcc.Input(id=\"search_query\", type=\"text\", placeholder=\"search query\"),\n",
    "    html.Button('Search', id='search_submit'),\n",
    "    html.Div(id='search_time'),\n",
    "    html.H2(children='Output'),\n",
    "    dash_table.DataTable(\n",
    "        data=[],\n",
    "        columns=search_output_columns,\n",
    "        id='search_output',\n",
    "        style_cell={'whiteSpace': 'normal', 'height': 'auto'},\n",
    "        style_data_conditional=[{'whiteSpace': 'normal', 'height': 'auto'}]\n",
    "    ),\n",
    "    html.H2(children='Users'),\n",
    "    dash_table.DataTable(\n",
    "        data=pd.DataFrame().to_dict('records'),\n",
    "        columns=user_df_columns,\n",
    "        id='user_df',\n",
    "        style_cell={'whiteSpace': 'normal', 'height': 'auto'},\n",
    "        style_data_conditional=[{'whiteSpace': 'normal', 'height': 'auto'}]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "@callback(\n",
    "    Output('search_output', 'data'),\n",
    "    Output('user_df', 'data'),\n",
    "    Output('search_time', 'children'),\n",
    "    State('search_selection', 'value'),\n",
    "    State('search_query', 'value'),\n",
    "    Input('search_submit', 'n_clicks')\n",
    ") # This function should be updated to refer to and call tweets as appropriate\n",
    "def update_table(stype, value, n):\n",
    "    start = time.time()\n",
    "    print(start)\n",
    "    dff = PC.keyword(value, stype)\n",
    "    print('Got tweets')\n",
    "    print(time.time())\n",
    "    dff2 = PC.userlist(dff)\n",
    "    print('Got users')\n",
    "    print(time.time())\n",
    "    \n",
    "    end = time.time()\n",
    "    return dff.reset_index().to_dict(\"records\"), dff2.reset_index().to_dict(\"records\"), \"Search Time: {} s\".format(end-start, 3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf814a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
